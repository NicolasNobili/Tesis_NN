# ───────────────────────────────────────────────────────────────────────────────
# 📦 Standard Library Imports
# ───────────────────────────────────────────────────────────────────────────────
import os                  
from pathlib import Path    # For file and directory manipulation
import sys                  # To modify Python path for custom module imports
import csv                  # To handle CSV file reading/writing
import random               # For generating random numbers
import numpy as np          # Numerical operations and array handling
import pandas as pd         # DataFrame handling for structured data
import matplotlib.pyplot as plt  # Plotting and visualization


# ───────────────────────────────────────────────────────────────────────────────
# 🌍 Third-Party Library Imports
# ───────────────────────────────────────────────────────────────────────────────
import torch                # PyTorch: deep learning framework
import geopandas as gpd     # For handling geospatial data with GeoDataFrames

# ───────────────────────────────────────────────────────────────────────────────
# 🧩 Custom Project Modules
# ───────────────────────────────────────────────────────────────────────────────

# Add custom project folder to system path to enable local module imports
sys.path.append('C:/Users/nnobi/Desktop/FIUBA/Tesis/Project')

# Import common training routines
from project_package.utils import train_common_routines as tcr

# Import model
from project_package.conv_net.ConvNet_model import SRCNN

# Import dataset manager
from project_package.dataset_manager.tensor_images_dataset import Tensor_images_dataset

# Import Sentinel-2 to Venus preprocessing utilities
from project_package.data_processing import sen2venus_routines as s2v

# Import general utility functions
from project_package.utils import utils as utils



#Select GPUs is they are available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device: ",device)


#Parameters for training
patched_images = 0
model_selection='large'
epochs = 200 # number of epochs to train the SRCNN model for
lr = 0.00001 # the learning rate
batch_size = 32
patch_size=64
stride=48
scale_value=10000

#Split porcentages for training, validation and testing
train_data_ratio=0.95
validation_data_ratio=0.04
test_data_ratio= 1 - train_data_ratio - validation_data_ratio


#Training data folder generated by load_files_tensor_data. It is in .pt files ready to be loaded 
data_folder = 'C:/Users/nnobi/Desktop/FIUBA/Tesis/Project/datasets/my_dataset2'
#Folder where results will be save
results_folder = 'C:/Users/nnobi/Desktop/FIUBA/Tesis/Project/results/Conv_Net'
# Crear carpeta de resultados si no existe
os.makedirs(results_folder, exist_ok=True)

# Files to be saved
# Archivos a guardar
loss_png_file = os.path.join(results_folder, f"loss_lr={lr}_batch_size={batch_size}_model={model_selection}_patched_images={patched_images}.png")
psnr_png_file = os.path.join(results_folder, f"psnr_lr={lr}_batch_size={batch_size}_model={model_selection}_patched_images={patched_images}.png")
final_model_pth_file = os.path.join(results_folder, f"model_lr={lr}_batch_size={batch_size}_model={model_selection}_patched_images={patched_images}.pth")
file_training_losses = os.path.join(results_folder, f"training_losses_lr={lr}_batch_size={batch_size}_model={model_selection}_patched_images={patched_images}.csv")

#Model initialization

model = SRCNN(model_selection).to(device)
print("The model: ")
print(model)
# Get parameter count
model.count_parameters()
print(f"Total Parameters: {model.total_params:,}")
print(f"Trainable Parameters: {model.trainable_params:,}")
#Load model on multiple GPUs if available
model=tcr.multi_GPU_training(model)