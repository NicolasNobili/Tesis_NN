{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1463,"status":"ok","timestamp":1748877362649,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"},"user_tz":180},"id":"HpomfX25xykJ","outputId":"9584ebb1-9e3f-4623-fdd4-7a869c7d8b3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive montado. Carpeta repo seteada en: /content/drive/MyDrive/Tesis/Tesis_NN\n"]}],"source":["from google.colab import drive\n","import sys\n","import os\n","\n","# Montar Google Drive\n","drive.mount('/content/drive')\n","\n","# Definir rutas base y repo\n","BASE_PATH = '/content/drive/MyDrive/Tesis'\n","REPO_NAME = 'Tesis_NN'\n","REPO_PATH = os.path.join(BASE_PATH, REPO_NAME)\n","\n","# Agregar repo al path de Python para imports\n","if REPO_PATH not in sys.path:\n","    sys.path.append(REPO_PATH)\n","\n","# Cambiar directorio actual al repo (para comandos git)\n","os.chdir(REPO_PATH)\n","\n","print(f\"Drive montado. Carpeta repo seteada en: {REPO_PATH}\")"]},{"cell_type":"code","source":["from google.colab import files\n","%cd /content/drive/MyDrive/Tesis/Tesis_NN\n","!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1VfCcaSOEF7M","executionInfo":{"status":"ok","timestamp":1748877373645,"user_tz":180,"elapsed":10993,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"3c747f5c-eef0-49bf-d797-eb2ef480416b"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Collecting asttokens==3.0.0 (from -r requirements.txt (line 1))\n","  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: braceexpand==0.1.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.1.7)\n","Requirement already satisfied: certifi==2025.4.26 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2025.4.26)\n","Collecting colorama==0.4.6 (from -r requirements.txt (line 4))\n","  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Collecting comm==0.2.2 (from -r requirements.txt (line 5))\n","  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: contourpy==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.3.2)\n","Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.12.1)\n","Collecting debugpy==1.8.14 (from -r requirements.txt (line 8))\n","  Using cached debugpy-1.8.14-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Collecting decorator==5.2.1 (from -r requirements.txt (line 9))\n","  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n","Collecting executing==2.2.0 (from -r requirements.txt (line 10))\n","  Using cached executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n","Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.18.0)\n","Collecting fonttools==4.58.1 (from -r requirements.txt (line 12))\n","  Using cached fonttools-4.58.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (106 kB)\n","Collecting fsspec==2025.5.1 (from -r requirements.txt (line 13))\n","  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: geopandas==1.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (1.0.1)\n","Collecting ipykernel==6.29.5 (from -r requirements.txt (line 15))\n","  Using cached ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n","Collecting ipython==9.3.0 (from -r requirements.txt (line 16))\n","  Using cached ipython-9.3.0-py3-none-any.whl.metadata (4.4 kB)\n","Collecting ipython_pygments_lexers==1.1.1 (from -r requirements.txt (line 17))\n","  Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n","Collecting jedi==0.19.2 (from -r requirements.txt (line 18))\n","  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (3.1.6)\n","Collecting jupyter_client==8.6.3 (from -r requirements.txt (line 20))\n","  Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n","Collecting jupyter_core==5.8.1 (from -r requirements.txt (line 21))\n","  Using cached jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: kiwisolver==1.4.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (1.4.8)\n","Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (3.0.2)\n","Collecting matplotlib==3.10.3 (from -r requirements.txt (line 24))\n","  Using cached matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (0.1.7)\n","Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (1.3.0)\n","Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (1.6.0)\n","Collecting networkx==3.5 (from -r requirements.txt (line 28))\n","  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n","Collecting numpy==2.2.6 (from -r requirements.txt (line 29))\n","  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","Collecting packaging==25.0 (from -r requirements.txt (line 30))\n","  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n","Collecting pandas==2.2.3 (from -r requirements.txt (line 31))\n","  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n","Requirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (0.8.4)\n","Requirement already satisfied: pillow==11.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (11.2.1)\n","Requirement already satisfied: platformdirs==4.3.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (4.3.8)\n","Requirement already satisfied: prompt_toolkit==3.0.51 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (3.0.51)\n","Collecting psutil==7.0.0 (from -r requirements.txt (line 36))\n","  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n","Collecting pure_eval==0.2.3 (from -r requirements.txt (line 37))\n","  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: Pygments==2.19.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (2.19.1)\n","Requirement already satisfied: pyogrio==0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (0.11.0)\n","Requirement already satisfied: pyparsing==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (3.2.3)\n","Requirement already satisfied: pyproj==3.7.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (3.7.1)\n","Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (2.9.0.post0)\n","Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (2025.2)\n","\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==310 (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==310\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install webdataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yR34MitpFxA-","executionInfo":{"status":"ok","timestamp":1748877380736,"user_tz":180,"elapsed":4737,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"7e9bb304-5c01-4874-a9cc-bbecd6083129"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: webdataset in /usr/local/lib/python3.11/dist-packages (0.2.111)\n","Requirement already satisfied: braceexpand in /usr/local/lib/python3.11/dist-packages (from webdataset) (0.1.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from webdataset) (2.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from webdataset) (6.0.2)\n"]}]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1748877380762,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"},"user_tz":180},"id":"wEswSVb9yfbG"},"outputs":[],"source":["# ───────────────────────────────────────────────────────────────────────────────\n","# 📦 Standard Library Imports\n","# ───────────────────────────────────────────────────────────────────────────────\n","import os\n","import sys\n","import time\n","import csv\n","import json\n","from pathlib import Path\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 📚 Scientific & Data Libraries\n","# ───────────────────────────────────────────────────────────────────────────────\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 🌍 Third-Party Library Imports (PyTorch)\n","# ───────────────────────────────────────────────────────────────────────────────\n","import torch\n","import torch.optim as optim\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 🧩 Custom Project Modules\n","# ───────────────────────────────────────────────────────────────────────────────\n","sys.path.append('C:/Users/nnobi/Desktop/FIUBA/Tesis/Project')\n","\n","from project_package.utils import train_common_routines as tcr\n","from project_package.conv_net.ConvNet_model import SRCNN\n","from project_package.dataset_manager.webdataset_dataset import PtWebDataset\n","\n"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1748877385026,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"},"user_tz":180},"id":"5CYJUmCuzCvb"},"outputs":[],"source":["# ───────────────────────────────────────────────────────────────────────────────\n","# 🔧 Configuration\n","# ───────────────────────────────────────────────────────────────────────────────\n","model_selection = 'large'\n","epochs = 200\n","lr = 1e-5\n","batch_size = 32\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 📁 Paths Setup\n","# ───────────────────────────────────────────────────────────────────────────────\n","dataset = 'dataset2' # Select Dataset\n","dataset_folder = os.path.join(REPO_PATH, 'datasets', dataset)\n","# tar_path = \"file:\" + dataset_folder.replace(\"\\\\\", \"/\")\n","tar_path = dataset_folder.replace(\"\\\\\", \"/\")\n","metadata_path = os.path.join(dataset_folder, 'metadata.json')\n","# metadata_path = '/content/drive/MyDrive/Tesis/Tesis_NN/datasets/messi/metadata.json'\n","\n","with open(metadata_path, \"r\") as f:\n","    metadata = json.load(f)\n","\n","train_samples = metadata[\"splits\"][\"train\"][\"num_samples\"]\n","val_samples = metadata[\"splits\"][\"val\"][\"num_samples\"]\n","test_samples = metadata[\"splits\"][\"test\"][\"num_samples\"]\n","\n","# Results folder and files\n","results_folder = os.path.join(REPO_PATH, 'results', 'Conv_Net')\n","os.makedirs(results_folder, exist_ok=True)\n","\n","loss_png_file = os.path.join(results_folder, f\"loss_lr={lr}_batch_size={batch_size}_model={model_selection}.png\")\n","psnr_png_file = os.path.join(results_folder, f\"psnr_lr={lr}_batch_size={batch_size}_model={model_selection}.png\")\n","final_model_pth_file = os.path.join(results_folder, f\"model_lr={lr}_batch_size={batch_size}_model={model_selection}.pth\")\n","file_training_losses = os.path.join(results_folder, f\"training_losses_lr={lr}_batch_size={batch_size}_model={model_selection}.csv\")"]},{"cell_type":"code","source":["# ───────────────────────────────────────────────────────────────────────────────\n","# 🚀 Training Pipeline\n","# ───────────────────────────────────────────────────────────────────────────────\n","\n","# 🧠 Model Initialization\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","model = SRCNN(model_selection).to(device)\n","print(\"The model:\")\n","print(model)\n","\n","model.count_parameters()\n","print(f\"Total Parameters: {model.total_params:,}\")\n","print(f\"Trainable Parameters: {model.trainable_params:,}\")\n","\n","model = tcr.multi_GPU_training(model)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","# 📊 Dataset and DataLoaders\n","dataset_train = PtWebDataset(tar_path + \"/train-*.tar\", length=train_samples, batch_size=32, shuffle_buffer=10 * batch_size)\n","dataset_val = PtWebDataset(tar_path + '/val-*.tar', length=val_samples, batch_size=16, shuffle_buffer=2 * batch_size)\n","dataset_test = PtWebDataset(tar_path + '/test.tar', length=test_samples, batch_size=2, shuffle_buffer=2 * batch_size)\n","\n","dataloader_train = dataset_train.get_dataloader(num_workers=2)\n","dataloader_val = dataset_val.get_dataloader(num_workers=0)\n","dataloader_test = dataset_test.get_dataloader(num_workers=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tN3yQsrlE8-i","executionInfo":{"status":"ok","timestamp":1748877389248,"user_tz":180,"elapsed":13,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"38c0ec6f-989f-4ab7-a875-88a8255d59e2"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","The model:\n","SRCNN(\n","  (conv1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), padding_mode=replicate)\n","  (relu1): ReLU(inplace=True)\n","  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n","  (relu2): ReLU(inplace=True)\n","  (conv3): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n","  (relu3): ReLU(inplace=True)\n","  (conv4): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",")\n","Total Parameters: 427,139\n","Trainable Parameters: 427,139\n","Model training to be done in only one GPU!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:389: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/webdataset/pipeline.py:176: UserWarning: .with_length() only sets the value of __len__ for compatibility with some training environments. It does not change the number of samples in an epoch.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["import glob\n","print(glob.glob('/content/drive/MyDrive/Tesis/Tesis_NN/datasets/dataset2/train-*.tar'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SFuyDIfMBht","executionInfo":{"status":"ok","timestamp":1748877393145,"user_tz":180,"elapsed":52,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"2292bd1b-8615-40af-a4ab-0276b370bbcc"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/MyDrive/Tesis/Tesis_NN/datasets/dataset2/train-00000.tar', '/content/drive/MyDrive/Tesis/Tesis_NN/datasets/dataset2/train-00001.tar', '/content/drive/MyDrive/Tesis/Tesis_NN/datasets/dataset2/train-00003.tar', '/content/drive/MyDrive/Tesis/Tesis_NN/datasets/dataset2/train-00002.tar']\n"]}]},{"cell_type":"code","source":["hola = PtWebDataset(tar_path + \"/train-*.tar\", length=train_samples, batch_size=32, shuffle_buffer=10 * batch_size)\n","hola.tar_path"],"metadata":{"id":"8-xXqiMIM8tb","executionInfo":{"status":"ok","timestamp":1748877467789,"user_tz":180,"elapsed":17,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"cb5df246-7a62-499c-df95-2992e83c2b59","colab":{"base_uri":"https://localhost:8080/","height":105}},"execution_count":58,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:389: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/webdataset/pipeline.py:176: UserWarning: .with_length() only sets the value of __len__ for compatibility with some training environments. It does not change the number of samples in an epoch.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Tesis/Tesis_NN/datasets/dataset2/train-*.tar'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":58}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":192,"status":"error","timestamp":1748876596166,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"},"user_tz":180},"id":"yN_2UvdtzEPq","colab":{"base_uri":"https://localhost:8080/","height":998},"outputId":"900d4caa-d5ea-4855-8e10-af3c6ff6e318"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1 of 200\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 42, in fetch\n    data = next(self.dataset_iter)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/pipeline.py\", line 105, in iterator\n    for sample in self.iterator1():\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 799, in _batched\n    for sample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 358, in _shuffle\n    for sample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 701, in _map_tuple\n    for sample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 665, in _to_tuple\n    for sample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/compat.py\", line 322, in check_empty\n    for sample in source:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/tariterators.py\", line 236, in group_by_keys\n    for filesample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/tariterators.py\", line 182, in tar_file_expander\n    for source in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/cache.py\", line 179, in __call__\n    if self.handler(exn):\n       ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 147, in reraise_exception\n    raise exn\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/cache.py\", line 173, in __call__\n    stream = open(parsed.path, \"rb\")\n             ^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/Tesis/Tesis_NN/datasets/dataset2/train-*.tar'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-6650d9e05f21>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch + 1} of {epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_psnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_MSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch_psnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_MSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Tesis/Tesis_NN/project_package/utils/train_common_routines.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, compute_loss, device, n_samples)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mrunning_psnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {i} size: {len(batch[0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mlow_res_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/webdataset/pipeline.py\u001b[0m in \u001b[0;36miterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepetitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 42, in fetch\n    data = next(self.dataset_iter)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/pipeline.py\", line 105, in iterator\n    for sample in self.iterator1():\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 799, in _batched\n    for sample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 358, in _shuffle\n    for sample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 701, in _map_tuple\n    for sample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 665, in _to_tuple\n    for sample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/compat.py\", line 322, in check_empty\n    for sample in source:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/tariterators.py\", line 236, in group_by_keys\n    for filesample in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/tariterators.py\", line 182, in tar_file_expander\n    for source in data:\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/cache.py\", line 179, in __call__\n    if self.handler(exn):\n       ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/filters.py\", line 147, in reraise_exception\n    raise exn\n  File \"/usr/local/lib/python3.11/dist-packages/webdataset/cache.py\", line 173, in __call__\n    stream = open(parsed.path, \"rb\")\n             ^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/Tesis/Tesis_NN/datasets/dataset2/train-*.tar'\n"]}],"source":["# 🏋️ Training Loop\n","train_loss, val_loss = [], []\n","train_psnr, val_psnr = [], []\n","\n","start = time.time()\n","for epoch in range(epochs):\n","    print(f\"\\nEpoch {epoch + 1} of {epochs}\")\n","\n","    train_epoch_loss, train_epoch_psnr = tcr.train(model, dataloader_train, optimizer, tcr.compute_loss_MSE, device, train_samples)\n","    val_epoch_loss, val_epoch_psnr = tcr.validate(model, dataloader_val, epoch, tcr.compute_loss_MSE, device, val_samples)\n","\n","    print(f\"Train PSNR: {train_epoch_psnr:.3f}\")\n","    print(f\"Val PSNR: {val_epoch_psnr:.3f}\")\n","\n","    train_loss.append(train_epoch_loss)\n","    train_psnr.append(train_epoch_psnr)\n","    val_loss.append(val_epoch_loss)\n","    val_psnr.append(val_epoch_psnr)\n","\n","    if epoch % 5 == 0:\n","        checkpoint_path = os.path.join(results_folder, f\"checkpoint_epoch_{epoch}_lr={lr}_batch_size={batch_size}_model={model_selection}.pth\")\n","        tcr.save_checkpoint(epoch, model, optimizer, train_loss, filename=checkpoint_path)\n","\n","    with open(file_training_losses, mode=\"a\", newline=\"\") as file:\n","        writer = csv.writer(file)\n","        writer.writerow([train_epoch_loss, train_epoch_psnr, val_epoch_loss, val_epoch_psnr])\n","\n","end = time.time()\n","print(f\"\\n✅ Finished training in: {(end - start) / 60:.2f} minutes\")\n","\n","# 📈 Loss Plot\n","plt.figure(figsize=(10, 7))\n","plt.plot(10 * np.log10(train_loss), color='orange', label='Train Loss')\n","plt.plot(10 * np.log10(val_loss), color='red', label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss (dB)')\n","plt.legend()\n","if os.path.exists(loss_png_file):\n","    os.remove(loss_png_file)\n","plt.savefig(loss_png_file)\n","plt.show()\n","\n","# 📈 PSNR Plot\n","plt.figure(figsize=(10, 7))\n","plt.plot(train_psnr, color='green', label='Train PSNR (dB)')\n","plt.plot(val_psnr, color='blue', label='Validation PSNR (dB)')\n","plt.xlabel('Epochs')\n","plt.ylabel('PSNR (dB)')\n","plt.legend()\n","if os.path.exists(psnr_png_file):\n","    os.remove(psnr_png_file)\n","plt.savefig(psnr_png_file)\n","plt.show()\n","\n","# 💾 Save Final Model\n","print('\\n💾 Saving model...')\n","if os.path.exists(final_model_pth_file):\n","    os.remove(final_model_pth_file)\n","model = model.module if hasattr(model, \"module\") else model\n","torch.save(model.state_dict(), final_model_pth_file)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}