{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2499,
     "status": "ok",
     "timestamp": 1748488488719,
     "user": {
      "displayName": "Nicolas Nobili",
      "userId": "17230013244848673171"
     },
     "user_tz": 180
    },
    "id": "HpomfX25xykJ",
    "outputId": "cf247c41-44da-42ce-cf1d-cf83b338001a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Drive montado. Carpeta repo seteada en: /content/drive/MyDrive/Tesis/Tesis_NN\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Montar Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Definir rutas base y repo\n",
    "BASE_PATH = '/content/drive/MyDrive/Tesis'\n",
    "REPO_NAME = 'Tesis_NN'\n",
    "REPO_PATH = os.path.join(BASE_PATH, REPO_NAME)\n",
    "\n",
    "# Agregar repo al path de Python para imports\n",
    "if REPO_PATH not in sys.path:\n",
    "    sys.path.append(REPO_PATH)\n",
    "\n",
    "# Cambiar directorio actual al repo (para comandos git)\n",
    "os.chdir(REPO_PATH)\n",
    "\n",
    "print(f\"Drive montado. Carpeta repo seteada en: {REPO_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8062,
     "status": "ok",
     "timestamp": 1748488496784,
     "user": {
      "displayName": "Nicolas Nobili",
      "userId": "17230013244848673171"
     },
     "user_tz": 180
    },
    "id": "wEswSVb9yfbG"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ“¦ Standard Library Imports\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ“š Scientific & Data Libraries\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸŒ Third-Party Library Imports (PyTorch)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ§© Custom Project Modules\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sys.path.append('C:/Users/nnobi/Desktop/FIUBA/Tesis/Project')\n",
    "\n",
    "from project_package.utils import train_common_routines as tcr\n",
    "from project_package.conv_net.ConvNet_model import SRCNN\n",
    "from project_package.dataset_manager.webdataset_dataset import PtWebDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1748488496888,
     "user": {
      "displayName": "Nicolas Nobili",
      "userId": "17230013244848673171"
     },
     "user_tz": 180
    },
    "id": "5CYJUmCuzCvb",
    "outputId": "caab511b-70be-409f-e3c5-f4105e40f440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ”§ Configuration\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model_selection = 'large'\n",
    "epochs = 200\n",
    "lr = 1e-5\n",
    "batch_size = 32\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ“ Paths Setup\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "project_dir = os.path.abspath(os.path.join(script_dir, '..'))\n",
    "\n",
    "dataset = 'messi' # Select Dataset\n",
    "dataset_folder = os.path.join(project_dir, 'datasets', dataset)\n",
    "tar_path = \"file:\" + dataset_folder.replace(\"\\\\\", \"/\")\n",
    "metadata_path = os.path.join(dataset_folder, 'metadata.json')\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "train_samples = metadata[\"splits\"][\"train\"][\"num_samples\"]\n",
    "val_samples = metadata[\"splits\"][\"val\"][\"num_samples\"]\n",
    "test_samples = metadata[\"splits\"][\"test\"][\"num_samples\"]\n",
    "\n",
    "# Results folder and files\n",
    "results_folder = os.path.join(project_dir, 'results', 'Conv_Net')\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "loss_png_file = os.path.join(results_folder, f\"loss_lr={lr}_batch_size={batch_size}_model={model_selection}.png\")\n",
    "psnr_png_file = os.path.join(results_folder, f\"psnr_lr={lr}_batch_size={batch_size}_model={model_selection}.png\")\n",
    "final_model_pth_file = os.path.join(results_folder, f\"model_lr={lr}_batch_size={batch_size}_model={model_selection}.pth\")\n",
    "file_training_losses = os.path.join(results_folder, f\"training_losses_lr={lr}_batch_size={batch_size}_model={model_selection}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1748488499758,
     "user": {
      "displayName": "Nicolas Nobili",
      "userId": "17230013244848673171"
     },
     "user_tz": 180
    },
    "id": "yN_2UvdtzEPq"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸš€ Training Pipeline\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# ğŸ§  Model Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model = SRCNN(model_selection).to(device)\n",
    "print(\"The model:\")\n",
    "print(model)\n",
    "\n",
    "model.count_parameters()\n",
    "print(f\"Total Parameters: {model.total_params:,}\")\n",
    "print(f\"Trainable Parameters: {model.trainable_params:,}\")\n",
    "\n",
    "model = tcr.multi_GPU_training(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# ğŸ“Š Dataset and DataLoaders\n",
    "dataset_train = PtWebDataset(tar_path + '/train.tar', length=train_samples, batch_size=batch_size, shuffle_buffer=5 * batch_size)\n",
    "dataset_val = PtWebDataset(tar_path + '/val.tar', length=val_samples, batch_size=batch_size, shuffle_buffer=5 * batch_size)\n",
    "dataset_test = PtWebDataset(tar_path + '/test.tar', length=test_samples, batch_size=batch_size, shuffle_buffer=5 * batch_size)\n",
    "\n",
    "dataloader_train = dataset_train.get_dataloader(num_workers=0)\n",
    "dataloader_val = dataset_val.get_dataloader(num_workers=0)\n",
    "dataloader_test = dataset_test.get_dataloader(num_workers=0)\n",
    "\n",
    "# ğŸ‹ï¸ Training Loop\n",
    "train_loss, val_loss = [], []\n",
    "train_psnr, val_psnr = [], []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1} of {epochs}\")\n",
    "\n",
    "    train_epoch_loss, train_epoch_psnr = tcr.train(model, dataloader_train, optimizer, tcr.compute_loss_MSE, device, train_samples)\n",
    "    val_epoch_loss, val_epoch_psnr = tcr.validate(model, dataloader_val, epoch, tcr.compute_loss_MSE, device, val_samples)\n",
    "\n",
    "    print(f\"Train PSNR: {train_epoch_psnr:.3f}\")\n",
    "    print(f\"Val PSNR: {val_epoch_psnr:.3f}\")\n",
    "\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_psnr.append(train_epoch_psnr)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_psnr.append(val_epoch_psnr)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        checkpoint_path = os.path.join(results_folder, f\"checkpoint_epoch_{epoch}_lr={lr}_batch_size={batch_size}_model={model_selection}.pth\")\n",
    "        tcr.save_checkpoint(epoch, model, optimizer, train_loss, filename=checkpoint_path)\n",
    "\n",
    "    with open(file_training_losses, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([train_epoch_loss, train_epoch_psnr, val_epoch_loss, val_epoch_psnr])\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nâœ… Finished training in: {(end - start) / 60:.2f} minutes\")\n",
    "\n",
    "# ğŸ“ˆ Loss Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(10 * np.log10(train_loss), color='orange', label='Train Loss')\n",
    "plt.plot(10 * np.log10(val_loss), color='red', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (dB)')\n",
    "plt.legend()\n",
    "if os.path.exists(loss_png_file):\n",
    "    os.remove(loss_png_file)\n",
    "plt.savefig(loss_png_file)\n",
    "plt.show()\n",
    "\n",
    "# ğŸ“ˆ PSNR Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_psnr, color='green', label='Train PSNR (dB)')\n",
    "plt.plot(val_psnr, color='blue', label='Validation PSNR (dB)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('PSNR (dB)')\n",
    "plt.legend()\n",
    "if os.path.exists(psnr_png_file):\n",
    "    os.remove(psnr_png_file)\n",
    "plt.savefig(psnr_png_file)\n",
    "plt.show()\n",
    "\n",
    "# ğŸ’¾ Save Final Model\n",
    "print('\\nğŸ’¾ Saving model...')\n",
    "if os.path.exists(final_model_pth_file):\n",
    "    os.remove(final_model_pth_file)\n",
    "model = model.module if hasattr(model, \"module\") else model\n",
    "torch.save(model.state_dict(), final_model_pth_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5141,
     "status": "ok",
     "timestamp": 1748488514649,
     "user": {
      "displayName": "Nicolas Nobili",
      "userId": "17230013244848673171"
     },
     "user_tz": 180
    },
    "id": "3VAXRuI1z7jI",
    "outputId": "64e244ca-e7e5-4cdb-bb55-c8f3f3e19d7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model: \n",
      "SRCNN(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), padding_mode=replicate)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (conv3): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (conv4): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      ")\n",
      "Total Parameters: 427,139\n",
      "Trainable Parameters: 427,139\n",
      "Model training to be done in only one GPU!\n",
      "Number of training samples: 123\n",
      "Number of validation samples: 5\n",
      "Number of testing samples: 1\n",
      "Training with unpatched images\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOesP92r6XUqlvraZNlLptt",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
