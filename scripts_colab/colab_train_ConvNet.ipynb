{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOPnfIlTgSrMCGKzpAVoPen"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HpomfX25xykJ","executionInfo":{"status":"ok","timestamp":1748484482795,"user_tz":180,"elapsed":25745,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"20b42183-58e4-4d58-d4ff-eb39124eb8d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Drive montado. Carpeta repo seteada en: /content/drive/MyDrive/Tesis/Tesis_NN\n"]}],"source":["from google.colab import drive\n","import sys\n","import os\n","\n","# Montar Google Drive\n","drive.mount('/content/drive')\n","\n","# Definir rutas base y repo\n","BASE_PATH = '/content/drive/MyDrive/Tesis'\n","REPO_NAME = 'Tesis_NN'\n","REPO_PATH = os.path.join(BASE_PATH, REPO_NAME)\n","\n","# Agregar repo al path de Python para imports\n","if REPO_PATH not in sys.path:\n","    sys.path.append(REPO_PATH)\n","\n","# Cambiar directorio actual al repo (para comandos git)\n","os.chdir(REPO_PATH)\n","\n","print(f\"Drive montado. Carpeta repo seteada en: {REPO_PATH}\")\n"]},{"cell_type":"code","source":["# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ“¦ Standard Library Imports\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import os\n","from pathlib import Path    # For file and directory manipulation\n","import sys                  # To modify Python path for custom module imports\n","import csv                  # To handle CSV file reading/writing\n","import random               # For generating random numbers\n","import numpy as np          # Numerical operations and array handling\n","import pandas as pd         # DataFrame handling for structured data\n","import matplotlib.pyplot as plt  # Plotting and visualization\n","import time\n","\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸŒ Third-Party Library Imports\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import torch                # PyTorch: deep learning framework\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torchvision.transforms.functional as functional_transforms\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ§© Custom Project Modules\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","# Add custom project folder to system path to enable local module imports\n","sys.path.append('C:/Users/nnobi/Desktop/FIUBA/Tesis/Project')\n","\n","# Import common training routines\n","from project_package.utils import train_common_routines as tcr\n","\n","# Import model\n","from project_package.conv_net.ConvNet_model import SRCNN\n","\n","# Import dataset manager\n","from project_package.dataset_manager.tensor_images_dataset import Tensor_images_dataset\n","\n","# Import Sentinel-2 to Venus preprocessing utilities\n","from project_package.data_processing import sen2venus_routines as s2v\n","\n","# Import general utility functions\n","from project_package.utils import utils as utils\n"],"metadata":{"id":"wEswSVb9yfbG","executionInfo":{"status":"ok","timestamp":1748484500014,"user_tz":180,"elapsed":13493,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#Select GPUs is they are available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device: \",device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CYJUmCuzCvb","executionInfo":{"status":"ok","timestamp":1748484503797,"user_tz":180,"elapsed":14,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"4f18c30d-9e47-4ef3-b2db-5aa4825bb848"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Device:  cuda\n"]}]},{"cell_type":"code","source":["#Dataset\n","dataset = 'my_dataset2'\n","\n","#Parameters for training\n","patched_images = 'no'\n","model_selection='large'\n","epochs = 200 # number of epochs to train the SRCNN model for\n","lr = 0.00001 # the learning rate\n","batch_size = 32\n","patch_size=64\n","stride=48\n","scale_value=10000\n","\n","#Split porcentages for training, validation and testing\n","train_data_ratio=0.95\n","validation_data_ratio=0.04\n","test_data_ratio= 1 - train_data_ratio - validation_data_ratio\n","\n","\n","#Training data folder generated by load_files_tensor_data. It is in .pt files ready to be loaded\n","data_folder = 'C:/Users/nnobi/Desktop/FIUBA/Tesis/Project/datasets/' + dataset\n","data_folder = os.path.join(REPO_PATH,'datasets',dataset)\n","#Folder where results will be save\n","results_folder = os.path.join(REPO_PATH,'Conv_Net')\n","# Crear carpeta de resultados si no existe\n","os.makedirs(results_folder, exist_ok=True)\n","\n","# Files to be saved\n","# Archivos a guardar\n","loss_png_file = os.path.join(results_folder, f\"loss_lr={lr}_batch_size={batch_size}_model={model_selection}_patched_images={patched_images}.png\")\n","psnr_png_file = os.path.join(results_folder, f\"psnr_lr={lr}_batch_size={batch_size}_model={model_selection}_patched_images={patched_images}.png\")\n","final_model_pth_file = os.path.join(results_folder, f\"model_lr={lr}_batch_size={batch_size}_model={model_selection}_patched_images={patched_images}.pth\")\n","file_training_losses = os.path.join(results_folder, f\"training_losses_lr={lr}_batch_size={batch_size}_model={model_selection}_patched_images={patched_images}.csv\")\n"],"metadata":{"id":"yN_2UvdtzEPq","executionInfo":{"status":"ok","timestamp":1748484737143,"user_tz":180,"elapsed":4,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#Model initialization\n","model = SRCNN(model_selection).to(device)\n","print(\"The model: \")\n","print(model)\n","# Get parameter count\n","model.count_parameters()\n","print(f\"Total Parameters: {model.total_params:,}\")\n","print(f\"Trainable Parameters: {model.trainable_params:,}\")\n","#Load model on multiple GPUs if available\n","model=tcr.multi_GPU_training(model)\n","\n","# optimizer\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","\n","if patched_images==\"no\":\n","    file_path_low_res = os.path.join(data_folder,'10m.pt')\n","    file_path_high_res= os.path.join(data_folder,'05m.pt')\n","\n","    #Initialize dataset for training\n","    dataset = Tensor_images_dataset(file_path_low_res,file_path_high_res)\n","\n","    #Split data\n","    train_data, val_data, test_data = tcr.data_split(dataset,train_data_ratio, validation_data_ratio, test_data_ratio)\n","\n","    # Create DataLoader (ensures batch-wise loading)\n","\n","    dataloader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","    dataloader_val = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","    dataloader_test = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","    print(\"Training with unpatched images\")\n","elif patched_images==\"yes\":\n","    #Create DataLoader for train data\n","    file_path_low_res_train=data_folder/f'train_data_low_res_patched_patch_size={patch_size}_stride={stride}.pt'\n","    file_path_high_res_train=data_folder/f'train_data_high_res_patched_patch_size={patch_size}_stride={stride}.pt'\n","    dataset_train = Tensor_images_dataset(file_path_low_res_train,file_path_high_res_train)\n","    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","\n","    #Create DataLoader for validation data\n","    file_path_low_res_val=data_folder/f'val_data_low_res_patched_patch_size={patch_size}_stride={stride}.pt'\n","    file_path_high_res_val=data_folder/f'val_data_high_res_patched_patch_size={patch_size}_stride={stride}.pt'\n","    dataset_val = Tensor_images_dataset(file_path_low_res_val,file_path_high_res_val)\n","    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","\n","    print(\"Training with patched images\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VAXRuI1z7jI","executionInfo":{"status":"ok","timestamp":1748484888292,"user_tz":180,"elapsed":16079,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"a3381081-30cc-474d-83b0-9877542022bb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["The model: \n","SRCNN(\n","  (conv1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), padding_mode=replicate)\n","  (relu1): ReLU(inplace=True)\n","  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n","  (relu2): ReLU(inplace=True)\n","  (conv3): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), padding_mode=replicate)\n","  (relu3): ReLU(inplace=True)\n","  (conv4): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",")\n","Total Parameters: 427,139\n","Trainable Parameters: 427,139\n","Model training to be done in only one GPU!\n","Number of training samples: 6842\n","Number of validation samples: 288\n","Number of testing samples: 72\n","Training with unpatched images\n"]}]},{"cell_type":"code","source":["#Begin training\n","train_loss, val_loss = [], []\n","train_psnr, val_psnr = [], []\n","start = time.time()\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch + 1} of {epochs}\")\n","    train_epoch_loss, train_epoch_psnr = tcr.train(model, dataloader_train, optimizer, tcr.compute_loss_MSE, device)\n","    val_epoch_loss, val_epoch_psnr = tcr.validate(model, dataloader_val, epoch, tcr.compute_loss_MSE, device)\n","    print(f\"Train PSNR: {train_epoch_psnr:.3f}\")\n","    print(f\"Val PSNR: {val_epoch_psnr:.3f}\")\n","    train_loss.append(train_epoch_loss)\n","    train_psnr.append(train_epoch_psnr)\n","    val_loss.append(val_epoch_loss)\n","    val_psnr.append(val_epoch_psnr)\n","    if epoch % 5 == 0:\n","        tcr.save_checkpoint(epoch, model, optimizer, train_loss, filename=results_folder/f\"checkpoint_epoch_{epoch}_lr={lr}_batch_size={batch_size}_model={model_selection}_patched_images={patched_images}.pth\")\n","    with open(file_training_losses, mode=\"a\", newline=\"\") as file:\n","        writer = csv.writer(file)\n","        writer.writerow([train_epoch_loss, train_epoch_psnr, val_epoch_loss, val_epoch_psnr])\n","end = time.time()\n","print(f\"Finished training in: {((end-start)/60):.3f} minutes\")\n","\n","\n","\n","#Plots training\n","plt.figure(figsize=(10, 7))\n","plt.plot(10*np.log10(train_loss), color='orange', label='train loss')\n","plt.plot(10*np.log10(val_loss), color='red', label='validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","if os.path.exists(loss_png_file):\n","    os.remove(loss_png_file)\n","plt.savefig(loss_png_file)\n","plt.show()\n","# psnr plots\n","plt.figure(figsize=(10, 7))\n","plt.plot(train_psnr, color='green', label='train PSNR dB')\n","plt.plot(val_psnr, color='blue', label='validation PSNR dB')\n","plt.xlabel('Epochs')\n","plt.ylabel('PSNR (dB)')\n","plt.legend()\n","if os.path.exists(psnr_png_file):\n","    os.remove(psnr_png_file)\n","plt.savefig(psnr_png_file)\n","plt.show()\n","\n","#Save the model to disk\n","\n","\n","print('Saving model...')\n","if os.path.exists(final_model_pth_file):\n","    os.remove(final_model_pth_file)\n","# If using DataParallel, remove it before saving\n","model = model.module if hasattr(model, \"module\") else model\n","torch.save(model.state_dict(), final_model_pth_file)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":738},"id":"QRJv7Q2X0BoW","executionInfo":{"status":"error","timestamp":1748484894265,"user_tz":180,"elapsed":803,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"01b0de29-44f6-4b50-ae25-acc4511b7b34"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 of 200\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/214 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/content/drive/MyDrive/Tesis/Tesis_NN/project_package/dataset_manager/tensor_images_dataset.py\", line 29, in __getitem__\n    resized_image_low_res=F.interpolate(self.data_low_res[idx].unsqueeze(0), size=(256, 256), mode='bicubic', align_corners=False).squeeze(0)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 4707, in interpolate\n    return torch._C._nn.upsample_bicubic2d(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: \"compute_indices_weights_cubic\" not implemented for 'Short'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-dd04d5d54ede>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1} of {epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_psnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_MSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_epoch_psnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_MSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train PSNR: {train_epoch_psnr:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Tesis/Tesis_NN/project_package/utils/train_common_routines.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, compute_loss, device)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mrunning_psnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mlow_res_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtruth_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/content/drive/MyDrive/Tesis/Tesis_NN/project_package/dataset_manager/tensor_images_dataset.py\", line 29, in __getitem__\n    resized_image_low_res=F.interpolate(self.data_low_res[idx].unsqueeze(0), size=(256, 256), mode='bicubic', align_corners=False).squeeze(0)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 4707, in interpolate\n    return torch._C._nn.upsample_bicubic2d(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: \"compute_indices_weights_cubic\" not implemented for 'Short'\n"]}]}]}