{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM6C2XVgPzXGknfXxZKUYlN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_tytkiCPSXfT","executionInfo":{"status":"ok","timestamp":1748526457794,"user_tz":180,"elapsed":62607,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"cc2d3e79-c941-4492-a914-40c2e517f8c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Drive montado. Carpeta repo seteada en: /content/drive/MyDrive/Tesis/Tesis_NN\n"]}],"source":["from google.colab import drive\n","import sys\n","import os\n","\n","# Montar Google Drive\n","drive.mount('/content/drive')\n","\n","# Definir rutas base y repo\n","BASE_PATH = '/content/drive/MyDrive/Tesis'\n","REPO_NAME = 'Tesis_NN'\n","REPO_PATH = os.path.join(BASE_PATH, REPO_NAME)\n","\n","# Agregar repo al path de Python para imports\n","if REPO_PATH not in sys.path:\n","    sys.path.append(REPO_PATH)\n","\n","# Cambiar directorio actual al repo (para comandos git)\n","os.chdir(REPO_PATH)\n","\n","print(f\"Drive montado. Carpeta repo seteada en: {REPO_PATH}\")\n"]},{"cell_type":"code","source":["# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ“¦ Standard Library Imports\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import os\n","from pathlib import Path    # For file and directory manipulation\n","import sys                  # To modify Python path for custom module imports\n","import csv                  # To handle CSV file reading/writing\n","import random               # For generating random numbers\n","import numpy as np          # Numerical operations and array handling\n","import pandas as pd         # DataFrame handling for structured data\n","import matplotlib.pyplot as plt  # Plotting and visualization\n","import time\n","\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸŒ Third-Party Library Imports\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import torch                # PyTorch: deep learning framework\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torchvision.transforms.functional as functional_transforms\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# ğŸ§© Custom Project Modules\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","\n","# Add custom project folder to system path to enable local module imports\n","sys.path.append('C:/Users/nnobi/Desktop/FIUBA/Tesis/Project')\n","\n","# Import common training routines\n","from project_package.utils import train_common_routines as tcr\n","\n","# Import model\n","from project_package.conv_net.ConvNet_model import SRCNN\n","\n","# Import dataset manager\n","from project_package.dataset_manager.tensor_images_dataset import Tensor_images_dataset\n","\n","# Import Sentinel-2 to Venus preprocessing utilities\n","from project_package.data_processing import sen2venus_routines as s2v\n","\n","# Import general utility functions\n","from project_package.utils import utils as utils\n"],"metadata":{"id":"fYh_vYvlSyNy","executionInfo":{"status":"ok","timestamp":1748526479561,"user_tz":180,"elapsed":16723,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install -q webdataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YIdRfJM_hwin","executionInfo":{"status":"ok","timestamp":1748530307192,"user_tz":180,"elapsed":3295,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"24dcadfd-4a28-49c9-9e89-1445f07ebf61"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/85.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import torch\n","\n","# CargÃ¡ el tensor\n","dataset = 'my_dataset3'\n","low_res_file = os.path.join(REPO_PATH,'datasets',dataset,'10m.pt')\n","high_res_file = os.path.join(REPO_PATH,'datasets',dataset,'05m.pt')\n","\n","\n","# Leer tensores\n","tensor_low_res = torch.load(low_res_file)\n","tensor_high_res = torch.load(high_res_file)\n","\n","# Castear a float32 y escalar\n","scale_value = 10000\n","tensor_low_res = tensor_low_res.float() / scale_value\n","tensor_high_res = tensor_high_res.float() / scale_value\n","\n","# NormalizaciÃ³n min-max por imagen y canal\n","max_val_low = tensor_low_res.amax(dim=(2, 3), keepdim=True)  # shape: [N, C, 1, 1]\n","min_val_low = tensor_low_res.amin(dim=(2, 3), keepdim=True)\n","tensor_low_res = (tensor_low_res - min_val_low) / (max_val_low - min_val_low + 1e-8)\n","\n","max_val_high = tensor_high_res.amax(dim=(2, 3), keepdim=True)\n","min_val_high = tensor_high_res.amin(dim=(2, 3), keepdim=True)\n","tensor_high_res = (tensor_high_res - min_val_high) / (max_val_high - min_val_high + 1e-8)\n","\n","# Interpolar low-res a 256x256 (bilinear)\n","tensor_low_res = F.interpolate(tensor_low_res, size=(256, 256), mode='bilinear', align_corners=False)\n","\n","\n","print(tensor_low_res.shape)\n","print(tensor_high_res.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxgJxRJoTMzS","executionInfo":{"status":"ok","timestamp":1748530215200,"user_tz":180,"elapsed":1713,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"36520105-9a4a-4946-abd8-be2a413eaeaa"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([129, 3, 256, 256])\n","torch.Size([129, 3, 256, 256])\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn.functional as F\n","import webdataset as wds\n","from pathlib import Path\n","import io\n","\n","def create_webdataset_shards_pt(tensor_low_res, tensor_high_res, output_dir, shard_size=1000):\n","    \"\"\"\n","    Guarda pares de tensores [N,3,H,W] low-res y high-res como archivos .pt dentro de shards WebDataset (.tar).\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    N = tensor_low_res.shape[0]\n","\n","    def save_shard(shard_idx, start_idx, end_idx):\n","        shard_path = os.path.join(output_dir, f\"shard-{shard_idx:05d}.tar\")\n","        with wds.TarWriter(shard_path) as sink:\n","            for i in range(start_idx, end_idx):\n","                sample_id = f\"{i:08d}\"\n","\n","                # Serializar tensor a bytes con torch.save\n","                low_buffer = io.BytesIO()\n","                torch.save(tensor_low_res[i], low_buffer)\n","                low_bytes = low_buffer.getvalue()\n","\n","                high_buffer = io.BytesIO()\n","                torch.save(tensor_high_res[i], high_buffer)\n","                high_bytes = high_buffer.getvalue()\n","\n","                sample = {\n","                    \"__key__\": sample_id,\n","                    \"low.pt\": low_bytes,\n","                    \"high.pt\": high_bytes,\n","                }\n","                sink.write(sample)\n","\n","    num_shards = (N + shard_size - 1) // shard_size\n","    for shard_idx in range(num_shards):\n","        start = shard_idx * shard_size\n","        end = min(start + shard_size, N)\n","        print(f\"Creando shard {shard_idx+1}/{num_shards} con Ã­ndices {start} a {end-1}\")\n","        save_shard(shard_idx, start, end)\n","\n","    print(\"Shards creados en:\", output_dir)\n","\n","\n","def torch_decoder(data):\n","    \"\"\"Decodificador para cargar tensores desde bytes en WebDataset\"\"\"\n","    buffer = io.BytesIO(data)\n","    return torch.load(buffer)\n","\n","\n","def get_webdataset_loader_pt(shard_pattern, batch_size=16, shuffle=True):\n","    \"\"\"\n","    Carga WebDataset con pares low.pt y high.pt, devuelve tensores en batches.\n","    \"\"\"\n","    dataset = (\n","        wds.WebDataset(shard_pattern)\n","        .decode(torch_decoder)\n","        .to_tuple(\"low.pt\", \"high.pt\")\n","    )\n","\n","    if shuffle:\n","        dataset = dataset.shuffle(1000)\n","\n","    loader = torch.utils.data.DataLoader(dataset.batched(batch_size), batch_size=None)\n","    return loader\n","\n","\n","# --- EJEMPLO de uso ---\n","\n","# Supongamos que tensor_low_res y tensor_high_res ya estÃ¡n creados con shape [N,3,256,256]\n","\n","BASE_PATH = \"/content/drive/MyDrive/Tesis\"  # CambiÃ¡ al path que uses\n","\n","# Crear shards (esto puede demorar un poco si N es grande)\n","create_webdataset_shards_pt(tensor_low_res, tensor_high_res, output_dir=os.path.join(BASE_PATH,'webdata_test'), shard_size=1000)\n","\n","import glob\n","\n","shard_paths = sorted(glob.glob(os.path.join(BASE_PATH, 'webdata_test', 'shard-*.tar')))\n","loader = get_webdataset_loader_pt(shard_paths, batch_size=8)\n","\n","for i, (low_batch, high_batch) in enumerate(loader):\n","    print(f\"Batch {i}: low_res {low_batch.shape}, high_res {high_batch.shape}\")\n","    # AquÃ­ podÃ©s usar tus batches para entrenar, validar, etc.\n","    if i == 1:  # solo para probar 2 batches\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"OQWV9-nOZH8V","executionInfo":{"status":"error","timestamp":1748533038004,"user_tz":180,"elapsed":926367,"user":{"displayName":"Nicolas Nobili","userId":"17230013244848673171"}},"outputId":"05c72976-573f-4645-9401-6e42cdabcbfc"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Creando shard 1/1 con Ã­ndices 0 a 128\n","Shards creados en: /content/drive/MyDrive/Tesis/webdata_test\n"]},{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-fc21c753539c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mshard_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'webdata_test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shard-*.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_webdataset_loader_pt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlow_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-fc21c753539c>\u001b[0m in \u001b[0;36mget_webdataset_loader_pt\u001b[0;34m(shard_pattern, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m     dataset = (\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWebDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"low.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"high.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/webdataset/compat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, urls, handler, mode, resampled, repeat, shardshuffle, cache_size, cache_dir, url_to_name, detshuffle, nodesplitter, workersplitter, select_files, rename_files, empty_check, verbose, seed)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# first, we add a generator for the urls to used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# this generates a stream of dict(url=...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_url_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# split by node (for distributed processing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/webdataset/compat.py\u001b[0m in \u001b[0;36mcreate_url_iterator\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshardlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResampledShardList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshardlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleShardList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/webdataset/shardlists.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, urls, seed)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]}]}