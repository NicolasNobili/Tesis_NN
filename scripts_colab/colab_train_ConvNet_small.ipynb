{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2634,
     "status": "ok",
     "timestamp": 1748882399526,
     "user": {
      "displayName": "Nicolas Nobili",
      "userId": "17230013244848673171"
     },
     "user_tz": 180
    },
    "id": "HpomfX25xykJ",
    "outputId": "30562b1f-3f5b-499b-90c1-fb279b808422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Drive montado. Carpeta repo seteada en: /content/drive/MyDrive/Tesis/Tesis_NN\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Montar Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Definir rutas base y repo\n",
    "BASE_PATH = '/content/drive/MyDrive/Tesis'\n",
    "REPO_NAME = 'Tesis_NN'\n",
    "REPO_PATH = os.path.join(BASE_PATH, REPO_NAME)\n",
    "\n",
    "# Agregar repo al path de Python para imports\n",
    "if REPO_PATH not in sys.path:\n",
    "    sys.path.append(REPO_PATH)\n",
    "\n",
    "# Cambiar directorio actual al repo (para comandos git)\n",
    "os.chdir(REPO_PATH)\n",
    "\n",
    "print(f\"Drive montado. Carpeta repo seteada en: {REPO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8959,
     "status": "ok",
     "timestamp": 1748882409407,
     "user": {
      "displayName": "Nicolas Nobili",
      "userId": "17230013244848673171"
     },
     "user_tz": 180
    },
    "id": "1VfCcaSOEF7M",
    "outputId": "e4e4d028-681d-4713-a7dd-78529eb8b3a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Collecting asttokens==3.0.0 (from -r requirements.txt (line 1))\n",
      "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: braceexpand==0.1.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.1.7)\n",
      "Requirement already satisfied: certifi==2025.4.26 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2025.4.26)\n",
      "Collecting colorama==0.4.6 (from -r requirements.txt (line 4))\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting comm==0.2.2 (from -r requirements.txt (line 5))\n",
      "  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: contourpy==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.12.1)\n",
      "Collecting debugpy==1.8.14 (from -r requirements.txt (line 8))\n",
      "  Using cached debugpy-1.8.14-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting decorator==5.2.1 (from -r requirements.txt (line 9))\n",
      "  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting executing==2.2.0 (from -r requirements.txt (line 10))\n",
      "  Using cached executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.18.0)\n",
      "Collecting fonttools==4.58.1 (from -r requirements.txt (line 12))\n",
      "  Using cached fonttools-4.58.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (106 kB)\n",
      "Collecting fsspec==2025.5.1 (from -r requirements.txt (line 13))\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: geopandas==1.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (1.0.1)\n",
      "Collecting ipykernel==6.29.5 (from -r requirements.txt (line 15))\n",
      "  Using cached ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting ipython==9.3.0 (from -r requirements.txt (line 16))\n",
      "  Using cached ipython-9.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting ipython_pygments_lexers==1.1.1 (from -r requirements.txt (line 17))\n",
      "  Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting jedi==0.19.2 (from -r requirements.txt (line 18))\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (3.1.6)\n",
      "Collecting jupyter_client==8.6.3 (from -r requirements.txt (line 20))\n",
      "  Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jupyter_core==5.8.1 (from -r requirements.txt (line 21))\n",
      "  Using cached jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: kiwisolver==1.4.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (1.4.8)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (3.0.2)\n",
      "Collecting matplotlib==3.10.3 (from -r requirements.txt (line 24))\n",
      "  Using cached matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (0.1.7)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (1.6.0)\n",
      "Collecting networkx==3.5 (from -r requirements.txt (line 28))\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy==2.2.6 (from -r requirements.txt (line 29))\n",
      "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting packaging==25.0 (from -r requirements.txt (line 30))\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pandas==2.2.3 (from -r requirements.txt (line 31))\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (0.8.4)\n",
      "Requirement already satisfied: pillow==11.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (11.2.1)\n",
      "Requirement already satisfied: platformdirs==4.3.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (4.3.8)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (3.0.51)\n",
      "Collecting psutil==7.0.0 (from -r requirements.txt (line 36))\n",
      "  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting pure_eval==0.2.3 (from -r requirements.txt (line 37))\n",
      "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: Pygments==2.19.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (2.19.1)\n",
      "Requirement already satisfied: pyogrio==0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (0.11.0)\n",
      "Requirement already satisfied: pyparsing==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (3.2.3)\n",
      "Requirement already satisfied: pyproj==3.7.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (3.7.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (2025.2)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==310 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==310\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "%cd /content/drive/MyDrive/Tesis/Tesis_NN\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4427,
     "status": "ok",
     "timestamp": 1748882416362,
     "user": {
      "displayName": "Nicolas Nobili",
      "userId": "17230013244848673171"
     },
     "user_tz": 180
    },
    "id": "yR34MitpFxA-",
    "outputId": "d0506ccb-5840-47c8-eb78-8d5df62108e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdataset in /usr/local/lib/python3.11/dist-packages (0.2.111)\n",
      "Requirement already satisfied: braceexpand in /usr/local/lib/python3.11/dist-packages (from webdataset) (0.1.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from webdataset) (2.0.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from webdataset) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748882419625,
     "user": {
      "displayName": "Nicolas Nobili",
      "userId": "17230013244848673171"
     },
     "user_tz": 180
    },
    "id": "OV7WdgICRDzb"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“¦ Standard Library Imports\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“š Scientific & Data Libraries\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸŒ Third-Party Library Imports (PyTorch)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ§© Custom Project Modules\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Add custom project folder to system path to enable local module imports\n",
    "if os.name == \"posix\":\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "else:\n",
    "    sys.path.append('C:/Users/nnobi/Desktop/FIUBA/Tesis/Project')\n",
    "\n",
    "from project_package.utils import train_common_routines as tcr\n",
    "from project_package.conv_net.ConvNet_model import SRCNN_small\n",
    "from project_package.dataset_manager.webdataset_dataset import PtWebDataset\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ”§ Configuration\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model_selection = 'SRCNN_small'\n",
    "epochs = 200\n",
    "lr = 1e-5\n",
    "batch_size = 32\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“ Paths Setup\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "script_dir = os.getcwd()\n",
    "project_dir = os.path.abspath(os.path.join(script_dir, '..'))\n",
    "\n",
    "dataset = 'dataset_test' # Select Dataset\n",
    "dataset_folder = os.path.join(project_dir, 'datasets', dataset)\n",
    "metadata_path = os.path.join(dataset_folder, 'metadata.json')\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "train_samples = metadata[\"splits\"][\"train\"][\"num_samples\"]\n",
    "val_samples = metadata[\"splits\"][\"val\"][\"num_samples\"]\n",
    "test_samples = metadata[\"splits\"][\"test\"][\"num_samples\"]\n",
    "\n",
    "# Results folder and files\n",
    "results_folder = os.path.join(project_dir, 'results', model_selection)\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "loss_png_file = os.path.join(results_folder, f\"loss_lr={lr}_batch_size={batch_size}_model={model_selection}.png\")\n",
    "psnr_png_file = os.path.join(results_folder, f\"psnr_lr={lr}_batch_size={batch_size}_model={model_selection}.png\")\n",
    "final_model_pth_file = os.path.join(results_folder, f\"model_lr={lr}_batch_size={batch_size}_model={model_selection}.pth\")\n",
    "file_training_losses = os.path.join(results_folder, f\"training_losses_lr={lr}_batch_size={batch_size}_model={model_selection}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸš€ Training Pipeline\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ðŸ§  Model Initialization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True # Enables cuDNN auto-tuner to find the best algorithm for fixed input sizes (improves performance)\n",
    "\n",
    "    model = SRCNN_small().to(device)\n",
    "    print(\"The model:\")\n",
    "    print(model)\n",
    "\n",
    "    model.count_parameters()\n",
    "    print(f\"Total Parameters: {model.total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {model.trainable_params:,}\")\n",
    "\n",
    "    model = tcr.multi_GPU_training(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # ðŸ“Š Dataset and DataLoaders\n",
    "    # Tune num_workers for dataloaders and buffersize\n",
    "    dataset_train = PtWebDataset(os.path.join(dataset_folder,'train-*.tar'), length=train_samples, batch_size=batch_size, shuffle_buffer=5 * batch_size)\n",
    "    dataset_val = PtWebDataset(os.path.join(dataset_folder,'val-*.tar'), length=val_samples, batch_size=batch_size, shuffle_buffer=5 * batch_size)\n",
    "    dataset_test = PtWebDataset(os.path.join(dataset_folder,'test.tar'), length=test_samples, batch_size=batch_size, shuffle_buffer=5 * batch_size)\n",
    "\n",
    "    dataloader_train = dataset_train.get_dataloader(num_workers=1)\n",
    "    dataloader_val = dataset_val.get_dataloader(num_workers=1)\n",
    "    dataloader_test = dataset_test.get_dataloader(num_workers=1)\n",
    "\n",
    "    # ðŸ‹ï¸ Training Loop\n",
    "    train_loss, val_loss = [], []\n",
    "    train_psnr, val_psnr = [], []\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1} of {epochs}\")\n",
    "\n",
    "        train_epoch_loss, train_epoch_psnr = tcr.train(model, dataloader_train, optimizer, tcr.compute_loss_MSE, device, train_samples)\n",
    "        val_epoch_loss, val_epoch_psnr = tcr.validate(model, dataloader_val, epoch, tcr.compute_loss_MSE, device, val_samples)\n",
    "\n",
    "        print(f\"Train PSNR: {train_epoch_psnr:.3f}\")\n",
    "        print(f\"Val PSNR: {val_epoch_psnr:.3f}\")\n",
    "\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_psnr.append(train_epoch_psnr)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "        val_psnr.append(val_epoch_psnr)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            checkpoint_path = os.path.join(results_folder, f\"checkpoint_epoch_{epoch}_lr={lr}_batch_size={batch_size}_model={model_selection}.pth\")\n",
    "            tcr.save_checkpoint(epoch, model, optimizer, train_loss, filename=checkpoint_path)\n",
    "\n",
    "        with open(file_training_losses, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([train_epoch_loss, train_epoch_psnr, val_epoch_loss, val_epoch_psnr])\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\nâœ… Finished training in: {(end - start) / 60:.2f} minutes\")\n",
    "\n",
    "    # ðŸ“ˆ Loss Plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(10 * np.log10(train_loss), color='orange', label='Train Loss')\n",
    "    plt.plot(10 * np.log10(val_loss), color='red', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (dB)')\n",
    "    plt.legend()\n",
    "    if os.path.exists(loss_png_file):\n",
    "        os.remove(loss_png_file)\n",
    "    plt.savefig(loss_png_file)\n",
    "    plt.show()\n",
    "\n",
    "    # ðŸ“ˆ PSNR Plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_psnr, color='green', label='Train PSNR (dB)')\n",
    "    plt.plot(val_psnr, color='blue', label='Validation PSNR (dB)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    plt.legend()\n",
    "    if os.path.exists(psnr_png_file):\n",
    "        os.remove(psnr_png_file)\n",
    "    plt.savefig(psnr_png_file)\n",
    "    plt.show()\n",
    "\n",
    "    # ðŸ’¾ Save Final Model\n",
    "    print('\\nðŸ’¾ Saving model...')\n",
    "    if os.path.exists(final_model_pth_file):\n",
    "        os.remove(final_model_pth_file)\n",
    "    model = model.module if hasattr(model, \"module\") else model\n",
    "    torch.save(model.state_dict(), final_model_pth_file)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
